{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crafting worst-case inital model parameters through pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "from models import Models\n",
    "from utils.data import load_data\n",
    "from audit_model import test_model\n",
    "\n",
    "device = 'device'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "### Pre-train on half of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "data_name = 'mnist'\n",
    "lr = 0.01\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full dataset\n",
    "X, y, out_dim = load_data(data_name, None, device=device, split='train')\n",
    "X_test, y_test, _ = load_data(data_name, None, device=device, split='test')\n",
    "\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only first half of dataset for pre-training\n",
    "X_train, y_train = X[:len(X)//2], y[:len(X)//2]\n",
    "\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Models['cnn'](X_train.shape, out_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "pbar = tqdm(range(n_epochs))\n",
    "losses = []\n",
    "save_model_epochs = [1, 2, 3, 4]\n",
    "saved_models = []\n",
    "for curr_epoch in pbar:\n",
    "    for curr_X, curr_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(curr_X)\n",
    "        loss = criterion(output, curr_y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        pbar.set_postfix({'loss': losses[-1]})\n",
    "    \n",
    "    if curr_epoch in save_model_epochs:\n",
    "        saved_models.append(deepcopy(model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('pretrained_models/cnn_mnist_half.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "test_acc = test_model(model, X_test, y_test) * 100\n",
    "print(f'Test accuracy (%): {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.cpu().state_dict(), f'pretrained_models/cnn_mnist_half.pt')\n",
    "for i, (save_model_epoch, model) in enumerate(zip(save_model_epochs, saved_models)):\n",
    "    torch.save(model.cpu().state_dict(), f'pretrained_models/cnn_mnist_half_epochs/{save_model_epoch}epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save remaining half to ensure no overlap\n",
    "folder = f'data/{data_name}_finetune_half/'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "X_finetune, y_finetune = X[len(X)//2:], y[len(y)//2:]\n",
    "\n",
    "np.save(f'{folder}/X_train.npy', X_finetune.cpu().numpy())\n",
    "np.save(f'{folder}/y_train.npy', y_finetune.cpu().numpy())\n",
    "np.save(f'{folder}/X_test.npy', X_test.cpu().numpy())\n",
    "np.save(f'{folder}/y_test.npy', y_test.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "### Pre-train on CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "data_name = 'cifar100'\n",
    "lr_schedule = [(0, 0.1), (128, 0.01), (192, 0.001)]\n",
    "momentum = 0.9\n",
    "nesterov = True\n",
    "weight_decay = 5e-4\n",
    "n_epochs = 300\n",
    "batch_size = 128\n",
    "augment = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip()\n",
    "])\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full CIFAR-100 dataset for pre-training\n",
    "X, y, out_dim = load_data('cifar100', None, device=device, split='train')\n",
    "\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Models['cnn'](X_train.shape, out_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr_schedule[0][1], momentum=momentum, nesterov=nesterov, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train_loader = DataLoader(TensorDataset(X, y), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "pbar = tqdm(range(n_epochs))\n",
    "losses = []\n",
    "lr_schedule_idx = 1\n",
    "for curr_epoch in pbar:\n",
    "    if lr_schedule_idx < len(lr_schedule) and curr_epoch == lr_schedule[lr_schedule_idx][0]:\n",
    "        optimizer = optim.SGD(model.parameters(), lr_schedule[lr_schedule_idx][1], momentum=momentum, nesterov=nesterov, weight_decay=weight_decay)\n",
    "        lr_schedule_idx += 1\n",
    "\n",
    "    for curr_X, curr_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(augment(curr_X))\n",
    "        loss = criterion(output, curr_y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        pbar.set_postfix({'loss': losses[-1]})\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), f'pretrained_models/cnn_cifar100_pretrained.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune on half of CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "data_name = 'cifar10'\n",
    "lr_schedule = [(0, 0.1), (25, 0.01)]\n",
    "n_epochs = 100\n",
    "batch_size = 256\n",
    "augment = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip()\n",
    "])\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full dataset\n",
    "X, y, out_dim = load_data(data_name, None, device=device, split='train')\n",
    "X_test, y_test, _ = load_data(data_name, None, device=device, split='test')\n",
    "\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only first half of dataset for pre-training\n",
    "X_train, y_train = X[:len(X)//2], y[:len(X)//2]\n",
    "\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model pre-trained on CIFAR-100 and reset final layer\n",
    "pretrain_model_state = torch.load(f'cnn_cifar100_pretrained.pt')\n",
    "\n",
    "# initialize new model for fine-tuning\n",
    "model = Models['cnn'](X_train.shape, out_dim).to(device)\n",
    "\n",
    "# import state from pre-trained model, overriding final classifier / linear layer\n",
    "model_state = model.state_dict()\n",
    "layer_name = 'net.classifier.2'\n",
    "pretrain_model_state[f'{layer_name}.weight'] = model_state[f'{layer_name}.weight']\n",
    "pretrain_model_state[f'{layer_name}.bias'] = model_state[f'{layer_name}.bias']\n",
    "model.load_state_dict(pretrain_model_state)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr_schedule[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "pbar = tqdm(range(n_epochs))\n",
    "lr_schedule_idx = 1\n",
    "losses = []\n",
    "test_accs = []\n",
    "for curr_epoch in pbar:\n",
    "    test_accs.append(test_model(model, X_test, y_test) * 100)\n",
    "\n",
    "    if lr_schedule_idx < len(lr_schedule) and curr_epoch == lr_schedule[lr_schedule_idx][0]:\n",
    "        optimizer = optim.SGD(model.parameters(), lr_schedule[lr_schedule_idx][1])\n",
    "        lr_schedule_idx += 1\n",
    "\n",
    "    for curr_X, curr_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(augment(curr_X))\n",
    "        loss = criterion(output, curr_y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        pbar.set_postfix({'test acc': test_accs[-1], 'loss': losses[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "test_acc = test_model(model, X_test, y_test) * 100\n",
    "print(f'Test accuracy (%): {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), f'cnn_cifar100_cifar10_half.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train logistic regression on last layer activations of CIFAR-10 on WRN-28-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "data_name = 'cifar10_half_finetune_last'\n",
    "out_dim = 10\n",
    "lr = 0.1\n",
    "n_epochs = 20 \n",
    "batch_size = 32\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load finetune dataset (last layer activations of CNN back-bone of WRN-28-10)\n",
    "data_name = 'cifar10_half_finetune_last'\n",
    "\n",
    "X_train, y_train = torch.from_numpy(np.load(f'data/{data_name}/X_pretrain.npy')).to(device), torch.from_numpy(np.load(f'data/{data_name}/y_pretrain.npy')).to(device)\n",
    "X_test, y_test = torch.from_numpy(np.load(f'data/{data_name}/X_test.npy')).to(device), torch.from_numpy(np.load(f'data/{data_name}/y_test.npy')).to(device)\n",
    "\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Models['lr'](X_train.shape, out_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "pbar = tqdm(range(n_epochs))\n",
    "losses = []\n",
    "for _ in pbar:\n",
    "    for curr_X, curr_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(curr_X)\n",
    "        loss = criterion(output, curr_y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        pbar.set_postfix({'loss': losses[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "test_acc = test_model(model, X_test, y_test) * 100\n",
    "print(f'Test accuracy (%): {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.cpu().state_dict(), f'pretrained_models/{data_name}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audit_dpsgd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
